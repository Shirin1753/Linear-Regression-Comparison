
This project demonstrates linear regression using three types of gradient descent:

-  **Batch Gradient Descent**
-  **Stochastic Gradient Descent (SGD)**
-  **Mini-Batch Gradient Descent**

Each method is compared visually by:
- Plotting cost over iterations
- Showing the final regression line

###  Dataset
Synthetic linear data with noise, generated via NumPy.

###  What's Covered
- Loss function for individual instances
- Theta updates during training
- Comparison of convergence behavior

###  Files
- `Linear Regression.ipynb`: Full code and visualizations
- `README.md`: Project explanation

###  Visualization Examples
- Cost function convergence per method
- Regression line fitted to data

###  Requirements
- Python 3
- NumPy
- Matplotlib

### Author
Shirin


